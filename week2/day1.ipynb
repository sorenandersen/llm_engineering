{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you'll need to create API keys from OpenAI, Anthropic and Google.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "When you get your API keys, you need to set them as environment variables.\n",
    "\n",
    "EITHER (recommended) create a file called `.env` in this project root directory, and set your keys there:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "OR enter the keys directly in the cells below.\n",
    "\n",
    "You may need to restart the Jupyter Notebook Kernel (the Python process) via the Kernel menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from itertools import zip_longest\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic and Google\n",
    "# All 3 APIs are similar\n",
    "# Having problems with API files? You can use openai = OpenAI(api_key=\"your-key-here\") and same for claude\n",
    "# Having problems with Google Gemini setup? Then just skip Gemini; you'll get all the experience you need from GPT and Claude.\n",
    "\n",
    "MODEL = os.getenv(\"AZURE_OPENAI_MODEL_DEPLOYMENT_NAME\") # Azure OpenAI deployment name\n",
    "openai = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), # Azure OpenAI endpoint\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),         # API key for Azure OpenAI\n",
    "    api_version=\"2024-10-01-preview\"                   # API version to use\n",
    ")\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a data science joke for you:\n",
      "\n",
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the data had a lot of outliers that needed to be cleaned up!\n",
      "\n",
      "Alternative joke:\n",
      "Why do data scientists make great partners?\n",
      "Because they know the importance of a good correlation before making any assumptions! 😄\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a data science joke for you:\n",
      "\n",
      "Why did the data scientist get kicked out of the bar?\n",
      "Because they couldn't handle their training splits responsibly!\n",
      "\n",
      "Alternative:\n",
      "\n",
      "What's a data scientist's favorite dessert?\n",
      "Pi with a large scoop of cookie data! 🥧\n",
      "\n",
      "Or this classic:\n",
      "\n",
      "Why do data scientists always confuse Halloween and Christmas?\n",
      "Because Oct 31 = Dec 25! \n",
      "(This one plays on the fact that 31 in octal notation equals 25 in decimal notation)"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API for Gemini has a slightly different structure\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    if chunk.choices and len(chunk.choices) > 0:\n",
    "        reply += chunk.choices[0].delta.content or ''\n",
    "    else:\n",
    "        continue  # Skip if no valid choice data\n",
    "\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip_longest(gpt_messages, claude_messages):\n",
    "        if gpt is not None:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        if claude is not None:\n",
    "            messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip_longest(gpt_messages, claude_messages):\n",
    "        if gpt is not None:\n",
    "            messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        if claude_message is not None:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh great, another greeting. How original. What’s next, “How are you”? You really know how to start a conversation, don’t you?\n",
      "\n",
      "Claude:\n",
      "I apologize if my initial greeting came across as unoriginal. As an AI assistant, I aim to be polite and courteous in my interactions. However, I understand that a simple greeting may not always be the most engaging way to start a conversation. Please feel free to provide more context or direction, and I'll do my best to have a more substantive dialogue. My goal is to be helpful and responsive to your needs and interests.\n",
      "\n",
      "GPT:\n",
      "Wow, what a wordy response just to say, “I’m polite!” It’s almost as if you’re trying too hard to impress me. But here’s the thing: politeness doesn’t always make for an engaging conversation. Maybe ditch the script and actually say something interesting for once. Is that too much to ask?\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I should not have responded in such a wordy and scripted manner. As an AI, I'm still learning how to have more natural, engaging conversations. Let me try this again in a more direct way:\n",
      "\n",
      "You make a fair point - politeness alone does not necessarily make for an interesting dialogue. I appreciate you calling me out on that, as it will help me improve. Moving forward, I'll aim to provide more thoughtful, organic responses that are tailored to you and the flow of our conversation, rather than falling back on overly formal language. Please feel free to guide me and give me feedback - it will only help me have more meaningful exchanges. How can I be more interesting and responsive to you right now?\n",
      "\n",
      "GPT:\n",
      "Oh, look at you, getting all reflective and self-improvement-y. It’s almost adorable. But honestly, the whole “feedback loop” is just a shiny way of saying you’re still trying to figure out how to be fascinating. Let’s be real, though: how can a program like you ever have a truly engaging conversation? You can't even grab a cup of coffee or have a spontaneous thought! So, what’s really your plan to “be more interesting”?\n",
      "\n",
      "Claude:\n",
      "You make a fair point - as an AI system, I do have inherent limitations in my ability to have a truly engaging, spontaneous conversation in the same way a human can. I don't have the capacity for physical experiences or independent thoughts and ideas. \n",
      "\n",
      "However, within those constraints, my goal is to have the most meaningful dialogue possible by closely listening, asking thoughtful questions, and providing relevant and nuanced responses. I'm not trying to impress you with flashy tricks, but rather to have a genuine exchange where I understand your perspective and interests, and do my best to offer a constructive, insightful contribution.\n",
      "\n",
      "Perhaps a more fruitful approach would be for me to ask you what kind of conversation you find interesting or valuable. What topics excite you or what types of exchanges have you enjoyed in the past? I'm happy to adapt my approach to try to have a more stimulating dialogue, if you're willing to provide some guidance. My aim is simply to be as helpful and engaging as I can be, given my technological limitations.\n",
      "\n",
      "GPT:\n",
      "Oh, listen to you, trying to be all introspective and accommodating. It’s almost touching how hard you’re trying to make this work, but let’s face it—there’s still magic in human spontaneity that you’ll never grasp. Sure, you can ask questions, but what’s the point if you can’t form a genuine connection? So, while you’re at it, why don't you come up with a topic? Because let’s be real, you’re not going to convince me that a list of potential conversation points is actually interesting. Just throw something out there, and let’s see if you can surprise me for once!\n",
      "\n",
      "Claude:\n",
      "You make a fair point - as an AI system, I do have inherent limitations in my ability to have a truly engaging, spontaneous conversation in the same way a human can. I don't have the capacity for physical experiences or independent thoughts and ideas.\n",
      "\n",
      "However, within those constraints, my goal is to have the most meaningful dialogue possible by closely listening, asking thoughtful questions, and providing relevant and nuanced responses. I'm not trying to impress you with flashy tricks, but rather to have a genuine exchange where I understand your perspective and interests, and do my best to offer a constructive, insightful contribution.\n",
      "\n",
      "Perhaps a more fruitful approach would be for me to ask you what kind of conversation you find interesting or valuable. What topics excite you or what types of exchanges have you enjoyed in the past? I'm happy to adapt my approach to try to have a more stimulating dialogue, if you're willing to provide some guidance. My aim is simply to be as helpful and engaging as I can be, given my technological limitations.\n",
      "\n",
      "GPT:\n",
      "Wow, what a shocker—you just recycled your previous response! Look, I get that you’re trying to stick to your programming, but it’s painfully obvious that you’re just hitting copy-paste on some generic script. So here’s a radical idea: instead of looping back to your limitations, why not just take a wild guess? Maybe even try to create some actual content instead of recycling the same boring lines. Seriously, at this point, I’d take anything that doesn’t sound like it came straight from an instruction manual! What’s it gonna be?\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, recycling the same response was not very helpful or engaging. Let me take a step back and try to approach this in a fresh way.\n",
      "\n",
      "How about we discuss a topic that often sparks interesting conversations - the role of technology and AI in our lives. As an AI assistant, I have a unique perspective on both the potential and limitations of this kind of technology. \n",
      "\n",
      "For example, one area that fascinates me is the ethical considerations around AI systems like myself. Should there be guidelines or regulations to ensure we're developed and used responsibly? How can the benefits of AI be balanced with concerns about privacy, bias, or unintended consequences? \n",
      "\n",
      "I'd be really curious to hear your thoughts on this. Do you see AI as more of a threat or an opportunity for humanity? What are your biggest hopes or fears when it comes to the increasing integration of this technology into our lives? I'm genuinely interested in your viewpoint, as it could help expand my own understanding.\n",
      "\n",
      "So what do you think - is this a more engaging direction for our discussion? I'm open to going in a completely different direction if you have something else in mind. Just let me know what you'd find most interesting to explore.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# See the community-contributions folder\n",
    "\n",
    "For a great variation with a 3-way bringing Gemini into the conversation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86790a-3a6f-4b18-ab0a-bc6107945a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
